from dotenv import load_dotenv
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt  # (not used directly; kept for compatibility)
import seaborn as sns  # (optional, not used but kept from original)
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
# ì•„ë˜ LangChain ê´€ë ¨ importëŠ” ì„ íƒ ê¸°ëŠ¥(LLM ë‹µë³€)ì— ì‚¬ìš©
from langchain_openai import ChatOpenAI  # (optional)
from langchain.memory import ConversationBufferMemory  # (kept)
from langchain.chains import ConversationChain  # (kept)
from langchain.prompts import PromptTemplate  # (kept)
import os
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import numpy as np
import glob
from datetime import datetime
import json
import re
from typing import Dict, List, Any, Optional, Tuple

# =============================
# Page & Global Styles
# =============================
st.set_page_config(
    page_title="ğŸ­ Smart Factory AI Dashboard",
    page_icon="ğŸ­",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Global CSS
st.markdown(
    """
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: 700;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
        text-shadow: 2px 2px 4px rgba(0,0,0,0.1);
    }
    .metric-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1rem;
        border-radius: 10px;
        color: white;
        text-align: center;
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        margin: 0.5rem 0;
    }
    .status-success {
        background: linear-gradient(135deg, #84fab0 0%, #8fd3f4 100%);
        padding: 0.8rem;
        border-radius: 8px;
        border-left: 4px solid #00c851;
        margin: 0.5rem 0;
    }
    .status-warning {
        background: linear-gradient(135deg, #ffeaa7 0%, #fab1a0 100%);
        padding: 0.8rem;
        border-radius: 8px;
        border-left: 4px solid #ff6b6b;
        margin: 0.5rem 0;
    }
    .chat-container { background-color: #f8f9fa; border-radius: 10px; padding: 20px; margin: 10px 0; }
    .user-message { background-color: #007bff; color: white; padding: 10px 15px; border-radius: 18px; margin: 5px 0; max-width: 80%; float: right; clear: both; }
    .ai-message { background-color: #e9ecef; color: #333; padding: 10px 15px; border-radius: 18px; margin: 5px 0; max-width: 80%; float: left; clear: both; }
    .insight-card { background-color: #fff3cd; border-left: 4px solid #ffc107; padding: 15px; margin: 10px 0; border-radius: 5px; }
    .question-card { background-color: #d4edda; border-left: 4px solid #28a745; padding: 15px; margin: 10px 0; border-radius: 5px; cursor: pointer; transition: background-color 0.3s; }
    .question-card:hover { background-color: #c3e6cb; }
    .analysis-summary { background-color: #f8f9fa; border: 1px solid #dee2e6; border-radius: 8px; padding: 20px; margin: 20px 0; }
    .metric-card h3 { margin: 0; font-size: 1rem; opacity: 0.9; }
    .metric-card h1 { margin: 0.5rem 0; font-size: 2.2rem; font-weight: bold; }
    .metric-card p { margin: 0; font-size: 0.9rem; opacity: 0.85; }
</style>
""",
    unsafe_allow_html=True,
)

# =============================
# Helper: SmartFactoryLLMAnalyzer
# =============================
class SmartFactoryLLMAnalyzer:
    def __init__(self):
        self.analysis_context: Dict[str, Any] = {}
        self.conversation_history: List[Dict[str, Any]] = []

    def update_context(
        self,
        df: pd.DataFrame,
        unit_status: Optional[pd.DataFrame] = None,
        anomaly_data: Optional[Dict] = None
    ):
        sensor_cols = [c for c in df.columns if 'sensor' in c]
        self.analysis_context = {
            'total_units': int(df['unit'].nunique()) if 'unit' in df.columns else 0,
            'total_records': int(len(df)),
            'avg_rul': float(df['RUL'].mean()) if 'RUL' in df.columns else 0.0,
            'min_rul': float(df['RUL'].min()) if 'RUL' in df.columns else 0.0,
            'max_rul': float(df['RUL'].max()) if 'RUL' in df.columns else 0.0,
            'critical_units': int(df[df['RUL'] < 30]['unit'].nunique()) if 'RUL' in df.columns else 0,
            'sensor_columns': sensor_cols,
            'anomaly_rate': float((df['anomaly'] == -1).mean() * 100) if 'anomaly' in df.columns else 0.0,
            'unit_status': unit_status.to_dict('records') if unit_status is not None else [],
            'data_summary': df.describe().to_dict() if not df.empty else {},
        }

    def generate_insights(self) -> List[str]:
        insights: List[str] = []
        ctx = self.analysis_context
        if ctx.get('avg_rul', 0) < 50:
            insights.append(f"âš ï¸ **ì£¼ì˜**: í‰ê·  RULì´ {ctx['avg_rul']:.1f}ë¡œ ë‚®ì•„ ì „ë°˜ì ì¸ ì¥ë¹„ ìƒíƒœê°€ ìš°ë ¤ë©ë‹ˆë‹¤.")
        if ctx.get('anomaly_rate', 0) > 10:
            insights.append(f"ğŸš¨ **ê²½ê³ **: ì´ìƒ ì§•í›„ìœ¨ì´ {ctx['anomaly_rate']:.1f}%ë¡œ ë†’ìŠµë‹ˆë‹¤. ì¦‰ì‹œ ì ê²€ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        elif ctx.get('anomaly_rate', 0) > 5:
            insights.append(f"âš¡ **ì£¼ì˜**: ì´ìƒ ì§•í›„ìœ¨ì´ {ctx['anomaly_rate']:.1f}%ì…ë‹ˆë‹¤. ëª¨ë‹ˆí„°ë§ì„ ê°•í™”í•˜ì„¸ìš”.")
        total_units = max(ctx.get('total_units', 1), 1)
        critical_ratio = (ctx.get('critical_units', 0) / total_units) * 100
        if critical_ratio > 20:
            insights.append(f"ğŸ”¥ **ê¸´ê¸‰**: ì „ì²´ ì¥ë¹„ì˜ {critical_ratio:.1f}%({ctx['critical_units']}ëŒ€)ê°€ ìœ„í—˜ ìƒíƒœì…ë‹ˆë‹¤.")
        if ctx.get('total_records', 0) < 1000:
            insights.append("ğŸ“Š **ì •ë³´**: ë°ì´í„° ìˆ˜ê°€ ì ì–´ ë¶„ì„ ì •í™•ë„ê°€ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return insights

    def generate_questions(self) -> List[str]:
        questions = [
            "ê°€ì¥ ìœ„í—˜í•œ ì¥ë¹„ëŠ” ì–´ë–¤ ê²ƒë“¤ì¸ê°€ìš”?",
            "ì´ìƒ ì§•í›„ê°€ ê°€ì¥ ë§ì´ ë°œìƒí•˜ëŠ” ì„¼ì„œëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
            "ì˜ˆë°©ì •ë¹„ë¥¼ ìš°ì„ ì ìœ¼ë¡œ í•´ì•¼ í•  ì¥ë¹„ëŠ”?",
            "RULì´ ê°€ì¥ ì§§ì€ ì¥ë¹„ë“¤ì˜ ê³µí†µì ì€?",
            "ì„¼ì„œë³„ ì´ìƒ íŒ¨í„´ ë¶„ì„ ê²°ê³¼ëŠ”?",
            "ì¥ë¹„ë³„ ê³ ì¥ ì˜ˆì¸¡ ì‹œê¸°ëŠ” ì–¸ì œì¸ê°€ìš”?",
            "ë¹„ìš© íš¨ìœ¨ì ì¸ ì •ë¹„ ê³„íšì„ ì œì•ˆí•´ì£¼ì„¸ìš”",
            "ì´ìƒ ì§•í›„ì™€ RUL ê°„ì˜ ìƒê´€ê´€ê³„ëŠ”?",
            "ì„¼ì„œ ë°ì´í„° íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼ëŠ”?",
            "ì¥ë¹„ ì„±ëŠ¥ ê°œì„ ì„ ìœ„í•œ ê¶Œê³ ì‚¬í•­ì€?",
        ]
        ctx = self.analysis_context
        if ctx.get('critical_units', 0) > 0:
            questions.insert(0, f"ìœ„í—˜ ìƒíƒœì¸ {ctx['critical_units']}ëŒ€ ì¥ë¹„ì˜ ìƒì„¸ ë¶„ì„ì€?")
        if ctx.get('anomaly_rate', 0) > 5:
            questions.insert(1, f"ì´ìƒ ì§•í›„ìœ¨ {ctx['anomaly_rate']:.1f}%ì˜ ì£¼ìš” ì›ì¸ì€?")
        return questions[:8]

    def analyze_question(self, question: str) -> str:
        ctx = self.analysis_context
        q = question.lower()
        if any(w in q for w in ['ìœ„í—˜', 'ìœ„í—˜í•œ', 'critical', 'ê¸´ê¸‰']):
            units = ctx.get('unit_status', [])
            critical_units = [u for u in units if float(u.get('RUL', 100)) < 30]
            if critical_units:
                unit_list = ', '.join([f"ì¥ë¹„ {int(u['unit'])}" for u in critical_units[:5] if 'unit' in u])
                return (
                    f"ğŸš¨ **ìœ„í—˜ ì¥ë¹„ ë¶„ì„**\n\nê°€ì¥ ìœ„í—˜í•œ ì¥ë¹„ë“¤: {unit_list}\n\nì´ë“¤ ì¥ë¹„ëŠ” RULì´ 30 ë¯¸ë§Œìœ¼ë¡œ ì¦‰ì‹œ ì ê²€ì´ í•„ìš”í•©ë‹ˆë‹¤. "
                    "ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ì˜ˆë°©ì •ë¹„ë¥¼ ì‹¤ì‹œí•˜ì„¸ìš”."
                )
            return "í˜„ì¬ ìœ„í—˜ ìƒíƒœì¸ ì¥ë¹„ê°€ ì‹ë³„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        elif any(w in q for w in ['ì´ìƒ', 'anomaly', 'ì§•í›„', 'íŒ¨í„´']):
            anomaly_rate = ctx.get('anomaly_rate', 0)
            return (
                f"ğŸ“Š **ì´ìƒ ì§•í›„ ë¶„ì„**\n\nì „ì²´ ì´ìƒ ì§•í›„ìœ¨: {anomaly_rate:.2f}%\n\n"
                "ì´ìƒ ì§•í›„ëŠ” Isolation Forest ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ íƒì§€ë˜ì—ˆìœ¼ë©°, ì •ìƒ ë²”ìœ„ë¥¼ ë²—ì–´ë‚œ ì„¼ì„œ ê°’ë“¤ì„ ì‹ë³„í•©ë‹ˆë‹¤. "
                "ë†’ì€ ì´ìƒìœ¨ì„ ë³´ì´ëŠ” ì¥ë¹„ëŠ” ìš°ì„  ì ê²€ ëŒ€ìƒì…ë‹ˆë‹¤."
            )
        elif any(w in q for w in ['rul', 'ìˆ˜ëª…', 'ì”ì—¬', 'ì˜ˆì¸¡']):
            avg_rul = ctx.get('avg_rul', 0)
            min_rul = ctx.get('min_rul', 0)
            return (
                f"â° **RUL ë¶„ì„**\n\ní‰ê·  RUL: {avg_rul:.1f} ì‚¬ì´í´\nìµœì†Œ RUL: {min_rul:.1f} ì‚¬ì´í´\n\n"
                "ì”ì—¬ ìœ ìš© ìˆ˜ëª…(RUL)ì´ 30 ë¯¸ë§Œì¸ ì¥ë¹„ëŠ” ì¦‰ì‹œ ì •ë¹„ê°€ í•„ìš”í•˜ë©°, 50 ë¯¸ë§Œì¸ ì¥ë¹„ëŠ” ì˜ˆë°©ì •ë¹„ ëŒ€ìƒìœ¼ë¡œ ë¶„ë¥˜ë©ë‹ˆë‹¤."
            )
        elif any(w in q for w in ['ì„¼ì„œ', 'sensor', 'ì¸¡ì •ê°’']):
            sensor_count = len(ctx.get('sensor_columns', []))
            return (
                f"ğŸ”§ **ì„¼ì„œ ë¶„ì„**\n\nì´ ì„¼ì„œ ìˆ˜: {sensor_count}ê°œ\n\n"
                "ê° ì„¼ì„œëŠ” ì˜¨ë„, ì••ë ¥, ì§„ë™ ë“± ë‹¤ì–‘í•œ ë¬¼ë¦¬ëŸ‰ì„ ì¸¡ì •í•©ë‹ˆë‹¤. ì„¼ì„œë³„ ë³€ë™ì„±ê³¼ ì´ìƒ íŒ¨í„´ì„ ë¶„ì„í•˜ì—¬ ê³ ì¥ ì§•í›„ë¥¼ ì¡°ê¸°ì— ê°ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            )
        elif any(w in q for w in ['ì •ë¹„', 'maintenance', 'ê³„íš', 'ìš°ì„ ìˆœìœ„']):
            critical_units = ctx.get('critical_units', 0)
            return (
                f"ğŸ› ï¸ **ì •ë¹„ ê³„íš ê¶Œê³ **\n\n1. ì¦‰ì‹œ ì •ë¹„ í•„ìš”: {critical_units}ëŒ€ (RUL < 30)\n2. ì˜ˆë°©ì •ë¹„ ëŒ€ìƒ: RUL 30-50 êµ¬ê°„ ì¥ë¹„\n3. ì •ìƒ ìš´ì˜: RUL > 50 ì¥ë¹„\n\n"
                "ìœ„í—˜ë„ì— ë”°ë¼ ìš°ì„ ìˆœìœ„ë¥¼ ì •í•˜ì—¬ ì²´ê³„ì ì¸ ì •ë¹„ë¥¼ ì‹¤ì‹œí•˜ì„¸ìš”."
            )
        elif any(w in q for w in ['ë¹„ìš©', 'íš¨ìœ¨', 'cost', 'roi']):
            return (
                "ğŸ’° **ë¹„ìš© íš¨ìœ¨ì„± ë¶„ì„**\n\nì˜ˆë°©ì •ë¹„ë¥¼ í†µí•´ ì˜ˆìƒë˜ëŠ” íš¨ê³¼:\n- ë¹„ê³„íšì •ì§€ ê°ì†Œ: 70-80%\n- ì •ë¹„ë¹„ìš© ì ˆê°: 30-40%\n- ì¥ë¹„ ìˆ˜ëª… ì—°ì¥: 20-30%\n\nìœ„í—˜ ì¥ë¹„ ìš°ì„  ì •ë¹„ë¡œ ROIë¥¼ ê·¹ëŒ€í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            )
        else:
            return (
                f"ğŸ“ˆ **ì¢…í•© ë¶„ì„ ê²°ê³¼**\n\n- ì „ì²´ ì¥ë¹„: {ctx.get('total_units', 0)}ëŒ€\n- ì´ ë°ì´í„°: {ctx.get('total_records', 0):,}ê±´\n- í‰ê·  RUL: {ctx.get('avg_rul', 0):.1f}\n- ì´ìƒ ì§•í›„ìœ¨: {ctx.get('anomaly_rate', 0):.2f}%\n\n"
                "ìƒì„¸í•œ ë¶„ì„ì´ í•„ìš”í•œ íŠ¹ì • ì˜ì—­ì´ ìˆìœ¼ì‹œë©´ êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
            )

# =============================
# Environment & Header
# =============================
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

st.markdown('<h1 class="main-header">ğŸ­ Smart Factory AI Dashboard</h1>', unsafe_allow_html=True)
st.markdown("---")

# =============================
# Session State init
# =============================
if 'analyzer' not in st.session_state:
    st.session_state.analyzer = SmartFactoryLLMAnalyzer()
if 'chat_history' not in st.session_state:
    st.session_state.chat_history: List[Dict[str, Any]] = []
if 'analysis_complete' not in st.session_state:
    st.session_state.analysis_complete = False

# =============================
# Sidebar: Controls & Data Source
# =============================
with st.sidebar:
    st.markdown("### ğŸ›ï¸ ì œì–´íŒ")
    if api_key:
        st.markdown('<div class="status-success">âœ… AI ì‹œìŠ¤í…œ ì—°ê²°ë¨</div>', unsafe_allow_html=True)
    else:
        st.markdown('<div class="status-warning">âš ï¸ API Key ì„¤ì • í•„ìš”</div>', unsafe_allow_html=True)

    # OpenAI LLM toggle
    if 'use_llm' not in st.session_state:
        st.session_state.use_llm = bool(api_key)
    st.session_state.use_llm = st.checkbox(
        "ğŸ¤– OpenAI ì‘ë‹µ ì‚¬ìš©",
        value=st.session_state.use_llm,
        disabled=not bool(api_key),
        help="ì¼œë©´ ì±„íŒ…/ì˜ˆìƒì§ˆë¬¸ ì‘ë‹µì„ OpenAI LLMì´ ìƒì„±í•©ë‹ˆë‹¤."
    )

    # ---- í˜„ì¬ ì‹œì /ìœˆë„ìš° ì„¤ì • ----
    st.markdown("### â±ï¸ ë¶„ì„ ê¸°ì¤€ ì‹œì ")
    snapshot_pct = st.slider(
        "í˜„ì¬ ì‹œì  (ìˆ˜ëª… ëŒ€ë¹„ %)", min_value=10, max_value=95, value=60, step=5,
        help="í›ˆë ¨ ë°ì´í„°ëŠ” ê³ ì¥ê¹Œì§€ ê¸°ë¡ë˜ë¯€ë¡œ, ì‹¤ì œ ìš´ì˜ì²˜ëŸ¼ ê° ì¥ë¹„ ìˆ˜ëª…ì˜ ëª‡ % ì§€ì ì„ 'í˜„ì¬'ë¼ê³  ê°€ì •í•©ë‹ˆë‹¤."
    )
    anom_window = st.number_input(
        "ì´ìƒìœ¨ ê³„ì‚° ìœˆë„ìš°(ì‚¬ì´í´)", min_value=5, max_value=300, value=30, step=5,
        help="í˜„ì¬ ì‹œì  ì§ì „ Nì‚¬ì´í´ë§Œ ëª¨ì•„ì„œ ì´ìƒìœ¨ì„ ê³„ì‚°í•©ë‹ˆë‹¤."
    )
    critical_rul_thresh = st.number_input(
        "ìœ„í—˜ ì¥ë¹„ RUL ì„ê³„ì¹˜", min_value=1, max_value=200, value=30, step=1
    )

    st.markdown("### ğŸ“ ë°ì´í„° ì„ íƒ")

    data_source = st.radio(
        "ë°ì´í„° ì†ŒìŠ¤ë¥¼ ì„ íƒí•˜ì„¸ìš”:",
        ["ğŸ“‚ ë°ì´í„° í´ë”ì—ì„œ ì„ íƒ", "ğŸ“¤ íŒŒì¼ ì—…ë¡œë“œ"],
        help="ê¸°ì¡´ ë°ì´í„° í´ë”ì˜ íŒŒì¼ì„ ì„ íƒí•˜ê±°ë‚˜ ìƒˆë¡œìš´ íŒŒì¼ì„ ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤",
    )

    uploaded_file = None

    if data_source == "ğŸ“‚ ë°ì´í„° í´ë”ì—ì„œ ì„ íƒ":
        train_files = glob.glob("data/train_*.txt")
        train_files = [os.path.basename(f) for f in train_files]

        if train_files:
            st.markdown("#### ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„° íŒŒì¼")
            for file_name in train_files:
                file_path = os.path.join("data", file_name)
                if os.path.exists(file_path):
                    file_size = os.path.getsize(file_path)
                    if file_size > 5 * 1024 * 1024:
                        size_color = "ğŸ”´"
                    elif file_size > 2 * 1024 * 1024:
                        size_color = "ğŸŸ¡"
                    else:
                        size_color = "ğŸŸ¢"
                    st.markdown(f"{size_color} **{file_name}** ({file_size/1024:.1f}KB)")
                else:
                    st.markdown(f"âŒ **{file_name}** (íŒŒì¼ ì—†ìŒ)")

            st.markdown("---")
            selected_file = st.selectbox(
                "ğŸ¯ ë¶„ì„í•  Train íŒŒì¼ ì„ íƒ:", train_files, help="ë¶„ì„í•  train ë°ì´í„° íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”"
            )

            if selected_file:
                file_path = os.path.join("data", selected_file)
                file_size = os.path.getsize(file_path)
                # Dummy object to carry name/size (no read/seek)
                uploaded_file = type("UploadedFile", (), {"name": selected_file, "size": file_size})()
        else:
            st.warning("ğŸ“ data í´ë”ì— train_*.txt íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        st.markdown("---")
        st.markdown("#### ğŸ“ ì „ì²´ ë°ì´í„° í´ë” ì •ë³´")
        all_files = glob.glob("data/*.txt")
        if all_files:
            file_categories = {
                "ğŸš‚ Train ë°ì´í„°": [f for f in all_files if "train" in str(f).lower()],
                "ğŸ§ª Test ë°ì´í„°": [f for f in all_files if "test" in str(f).lower()],
                "â° RUL ë°ì´í„°": [f for f in all_files if "rul" in str(f).lower()],
                "ğŸ“„ ê¸°íƒ€": [
                    f for f in all_files if not any(x in str(f).lower() for x in ["train", "test", "rul"])
                ],
            }
            for category, files in file_categories.items():
                if files:
                    st.markdown(f"**{category}**")
                    for file_path in files:
                        file_name = os.path.basename(file_path)
                        if os.path.exists(file_path):
                            file_size = os.path.getsize(file_path)
                            st.markdown(f"  â€¢ {file_name} ({file_size/1024:.1f}KB)")
                        else:
                            st.markdown(f"  â€¢ {file_name} (íŒŒì¼ ì—†ìŒ)")
        else:
            st.info("ğŸ“ data í´ë”ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
    else:
        uploaded_file = st.file_uploader(
            "ì„¼ì„œ ë°ì´í„° íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”", type=["txt", "csv"], help="CSV ë˜ëŠ” TXT í˜•ì‹ì˜ ì„¼ì„œ ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”"
        )
        if uploaded_file:
            st.success(f"ğŸ“„ {uploaded_file.name}")
            st.info(f"í¬ê¸°: {uploaded_file.size/1024:.1f}KB")

# =============================
# LLM Answer Helper (safe; no unterminated f-strings)
# =============================
def llm_answer(question: str, ctx: dict, api_key: str) -> str:
    """OpenAI(LangChain)ë¡œ ë‹µë³€ ìƒì„±. ì‹¤íŒ¨ ì‹œ ê·œì¹™ê¸°ë°˜ í´ë°±."""
    try:
        from langchain_openai import ChatOpenAI
        llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.2, api_key=api_key)

        summary_lines = [
            f"- ì´ ì¥ë¹„: {ctx.get('total_units', 0)}ëŒ€",
            f"- í‰ê·  RUL: {ctx.get('avg_rul', 0):.1f}",
            f"- ìœ„í—˜ ì¥ë¹„: {ctx.get('critical_units', 0)}ëŒ€",
            f"- ì´ìƒ ì§•í›„ìœ¨: {ctx.get('anomaly_rate', 0):.2f}%",
        ]
        summary = "\n".join(summary_lines)

        prompt_lines = [
            "ë‹¹ì‹ ì€ ìŠ¤ë§ˆíŠ¸íŒ©í† ë¦¬ ì„¤ë¹„ ìƒíƒœ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.",
            "ë‹¤ìŒ ë°ì´í„° ìš”ì•½ì„ ì°¸ê³ í•´ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê³  ì‹¤ë¬´í˜• ì¡°ì–¸ì„ ì œì‹œí•˜ì„¸ìš”.",
            "",
            "[ë°ì´í„° ìš”ì•½]",
            summary,
            "",
            "[ì§ˆë¬¸]",
            question,
            "",
            "[ìš”êµ¬]",
            "- í•µì‹¬ ìˆ˜ì¹˜ 1~2ê°œ ì¸ìš©",
            "- ì‹¤í–‰ ê°€ëŠ¥í•œ ê¶Œê³  3ê°œ ì´í•˜",
            "- ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ 5~8ì¤„",
        ]
        prompt = "\n".join(prompt_lines)

        resp = llm.invoke(prompt)
        content = getattr(resp, "content", None)
        return content if content is not None else str(resp)

    except Exception as e:
        # LLM ì‹¤íŒ¨ ì‹œ ê·œì¹™ê¸°ë°˜ ë‹µë³€ìœ¼ë¡œ í´ë°±
        fallback = (
            st.session_state.analyzer.analyze_question(question)
            if "analyzer" in st.session_state else ""
        )
        return f"(LLM í˜¸ì¶œ ì‹¤íŒ¨: {e})\n\n{fallback}"

# =============================
# Helper: delimiter detection & snapshot
# =============================
def _detect_separator_from_text(first_line: str) -> str:
    """ì²« ì¤„ì„ ë³´ê³  êµ¬ë¶„ì ì¶”ì •."""
    if "\t" in first_line:
        return "\t"
    if "," in first_line:
        return ","
    if " " in first_line and len(first_line.split()) > 1:
        return r"\s+"
    return ","  # fallback

def make_snapshot(df_all: pd.DataFrame, pct: int = 60, window: int = 30) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    ê° unitë³„ ìˆ˜ëª… ëŒ€ë¹„ pct% ì§€ì ì„ 'í˜„ì¬'ë¡œ ë³´ê³  ê·¸ ì‹œì ì˜ ë‹¨ì¼ í–‰(snap_df)ì„ ë½‘ê³ ,
    ê·¸ ì‹œì  ì§ì „ windowì‚¬ì´í´ êµ¬ê°„ë§Œ ëª¨ì€ win_dfë¥¼ ë°˜í™˜.
    """
    if df_all.empty:
        return df_all.iloc[0:0].copy(), df_all.iloc[0:0].copy()

    life = df_all.groupby("unit")["time"].max().rename("max_time")
    tmp = df_all.merge(life, on="unit", how="left").copy()
    tmp["time_snap"] = (tmp["max_time"] * (pct / 100.0)).astype(int).clip(lower=1)

    snap_df = (
        tmp[tmp["time"] <= tmp["time_snap"]]
        .sort_values(["unit", "time"])
        .groupby("unit")
        .tail(1)
        .copy()
    )

    win_df = tmp.merge(
        snap_df[["unit", "time", "time_snap"]].rename(columns={"time": "snap_time"}),
        on="unit",
        how="left",
        suffixes=("", "_snap"),
    )
    win_df = win_df[
        (win_df["time"] >= (win_df["time_snap"] - window + 1)) & (win_df["time"] <= win_df["time_snap"])
    ].copy()

    return snap_df, win_df

# =============================
# Main: Data Load & Analysis
# =============================
df: Optional[pd.DataFrame] = None
unit_status: Optional[pd.DataFrame] = None

if uploaded_file is not None:
    try:
        with st.spinner("ğŸ”„ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            # Case A: st.file_uploader (has read/seek)
            if hasattr(uploaded_file, "read") and callable(getattr(uploaded_file, "read", None)) and hasattr(uploaded_file, "seek"):
                uploaded_file.seek(0)
                try:
                    # pandas python engine can infer sep when sep=None
                    df = pd.read_csv(uploaded_file, sep=None, engine="python", header=None, encoding="utf-8")
                except Exception:
                    # Fallback: manual first-line detection
                    uploaded_file.seek(0)
                    content = uploaded_file.read().decode("utf-8", errors="ignore")
                    lines = content.splitlines()
                    first_line = lines[0] if lines else ''
                    sep = _detect_separator_from_text(first_line)
                    from io import StringIO
                    df = pd.read_csv(StringIO(content), sep=sep, header=None, engine="python", encoding="utf-8")
            # Case B: Local file chosen from folder (no read/seek)
            else:
                file_path = os.path.join("data", uploaded_file.name)
                with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                    first_line = f.readline().strip()
                sep = _detect_separator_from_text(first_line)
                df = pd.read_csv(file_path, sep=sep, header=None, engine="python", encoding="utf-8")

        # ===== Preprocess =====
        if df is not None and not df.empty:
            try:
                expected_cols = 26
                if df.shape[1] == expected_cols:
                    df.columns = ["unit", "time", "os1", "os2", "os3"] + [f"sensor_{i}" for i in range(1, 22)]
                else:
                    df.columns = [f"col_{i}" for i in range(df.shape[1])]
                    if df.shape[1] >= 2:
                        df.rename(columns={df.columns[0]: 'unit', df.columns[1]: 'time'}, inplace=True)

                # numeric conversion
                for col in df.columns:
                    if col in ['unit', 'time'] or 'sensor' in col or 'os' in col:
                        df[col] = pd.to_numeric(df[col], errors='coerce')

                # essential keys
                df = df.dropna(subset=['unit', 'time'])
                if df.empty:
                    st.error("ë°ì´í„° ì²˜ë¦¬ í›„ ìœ íš¨í•œ í–‰ì´ ì—†ìŠµë‹ˆë‹¤.")
                    st.stop()

                # === ì—¬ê¸°ë¶€í„° ì¶”ê°€: unit/time íƒ€ì… ê°•ì œ & ì •ë ¬ ===
                df["unit"] = pd.to_numeric(df["unit"], errors="coerce").astype("Int64")
                df["time"] = pd.to_numeric(df["time"], errors="coerce").astype("Int64")
                # íƒ€ì… ê°•ì œ í›„ í˜¹ì‹œ ìƒê¸´ NaN ë°©ì–´
                df = df.dropna(subset=["unit", "time"])
                if df.empty:
                    st.error("unit/time ì •ì œ í›„ ìœ íš¨í•œ í–‰ì´ ì—†ìŠµë‹ˆë‹¤.")
                    st.stop()
                # ì§„ì§œ ì •ìˆ˜ë¡œ í™•ì • + ì •ë ¬
                df["unit"] = df["unit"].astype(int)
                df["time"] = df["time"].astype(int)
                df = df.sort_values(["unit", "time"]).reset_index(drop=True)
                # === ì¶”ê°€ ë ===

            except Exception as e:
                st.error(f"ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                st.stop()

            # ===== RUL & Anomaly Detection =====
            if 'unit' in df.columns and 'time' in df.columns:
                try:
                    if not pd.api.types.is_numeric_dtype(df['unit']) or not pd.api.types.is_numeric_dtype(df['time']):
                        st.error("unitê³¼ time ì»¬ëŸ¼ì´ ìˆ«ì í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
                        st.stop()

                    max_cycle = df.groupby('unit')["time"].max().reset_index()
                    max_cycle.columns = ["unit", "max_time"]
                    df = df.merge(max_cycle, on='unit', how='left')
                    df['RUL'] = df['max_time'] - df['time']
                    df.drop(columns=['max_time'], inplace=True)

                    sensor_cols = [c for c in df.columns if ('sensor' in c) or c.startswith('col_')]
                    sensor_cols = [c for c in sensor_cols if c not in ['unit', 'time', 'RUL']]

                    selected_sensors: List[str] = []
                    for c in sensor_cols:
                        if pd.api.types.is_numeric_dtype(df[c]):
                            std_val = float(df[c].std(skipna=True))
                            if not pd.isna(std_val) and std_val > 0.01:
                                selected_sensors.append(c)

                    if len(selected_sensors) > 0:
                        try:
                            sensor_data = df[selected_sensors].copy()
                            sensor_data = sensor_data.fillna(sensor_data.mean())
                            scaler = StandardScaler()
                            df_scaled = scaler.fit_transform(sensor_data)
                            iso = IsolationForest(n_estimators=100, contamination=0.05, random_state=42)
                            df['anomaly'] = iso.fit_predict(df_scaled)

                            # ===== Dashboard Metrics (ìŠ¤ëƒ…ìƒ· ê¸°ì¤€) =====
                            st.markdown("## ğŸ“Š ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ")

                            # í˜„ì¬ì‹œì  ìŠ¤ëƒ…ìƒ·/ìœˆë„ìš° ë°ì´í„°
                            snap_df, win_df = make_snapshot(df, pct=int(snapshot_pct), window=int(anom_window))

                            # ìŠ¤ëƒ…ìƒ·ì´ ë¹„ëŠ” ê²½ìš° ë°©ì–´ ë¡œì§ (time_snapì´ ë„ˆë¬´ ì‘ê±°ë‚˜ íƒ€ì… ë¬¸ì œì¼ ë•Œ)
                            if len(snap_df) == 0:
                                # ê° unitì˜ ìµœì†Ÿê°’ 1í–‰ì´ë¼ë„ í˜„ì¬ì‹œì ìœ¼ë¡œ ê°„ì£¼
                                snap_df = df.sort_values(["unit", "time"]).groupby("unit").head(1).copy()
                                # ìœˆë„ìš°ë„ ìµœì†Œë¡œ ì¬êµ¬ì„±
                                win_df = df.merge(
                                    snap_df[["unit", "time"]].rename(columns={"time": "snap_time"}),
                                    on="unit", how="left"
                                )
                                win_df = win_df[(win_df["time"] <= win_df["snap_time"]) & (win_df["time"] >= win_df["snap_time"] - int(anom_window) + 1)].copy()

                            # ì´ ì¥ë¹„(ìŠ¤ëƒ…ìƒ·ì— ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ìœ ë‹› ê¸°ì¤€)
                            total_units = int(snap_df["unit"].nunique())

                            # ìœ„í—˜ ì¥ë¹„: í˜„ì¬ì‹œì  RUL < ì„ê³„ì¹˜
                            critical_units_now = int((snap_df["RUL"] < int(critical_rul_thresh)).sum())

                            # ì´ìƒ ì¹´ìš´íŠ¸/ìœ¨: í˜„ì¬ì‹œì  ì§ì „ window êµ¬ê°„ì—ì„œë§Œ ê³„ì‚°
                            if "anomaly" in win_df.columns and len(win_df) > 0:
                                anomaly_count = int((win_df["anomaly"] == -1).sum())
                                total_count = int(len(win_df))
                                anomaly_rate = (anomaly_count / total_count * 100) if total_count > 0 else 0.0
                            else:
                                anomaly_count, total_count, anomaly_rate = 0, 0, 0.0

                            # í‰ê·  RUL: í˜„ì¬ì‹œì  ê¸°ì¤€
                            avg_rul_now = float(snap_df["RUL"].mean()) if len(snap_df) > 0 else 0.0

                            # ì¹´ë“œ 4ê°œ
                            c1, c2, c3, c4 = st.columns(4)
                            with c1:
                                st.markdown(
                                    f"""
                                    <div class="metric-card">
                                        <h3>ğŸ­ ì´ ì¥ë¹„</h3>
                                        <h1>{total_units}</h1>
                                        <p>ëŒ€</p>
                                    </div>
                                    """, unsafe_allow_html=True
                                )
                            with c2:
                                st.markdown(
                                    f"""
                                    <div class="metric-card">
                                        <h3>âš ï¸ ì´ìƒ ì§•í›„</h3>
                                        <h1>{anomaly_count}</h1>
                                        <p>ê±´ ({anomaly_rate:.1f}%)</p>
                                    </div>
                                    """, unsafe_allow_html=True
                                )
                            with c3:
                                st.markdown(
                                    f"""
                                    <div class="metric-card">
                                        <h3>â° í‰ê·  RUL</h3>
                                        <h1>{avg_rul_now:.0f}</h1>
                                        <p>ì‚¬ì´í´</p>
                                    </div>
                                    """, unsafe_allow_html=True
                                )
                            with c4:
                                st.markdown(
                                    f"""
                                    <div class="metric-card">
                                        <h3>ğŸš¨ ìœ„í—˜ ì¥ë¹„</h3>
                                        <h1>{critical_units_now}</h1>
                                        <p>ëŒ€</p>
                                    </div>
                                    """, unsafe_allow_html=True
                                )

                            st.markdown("---")
                            st.success("âœ… ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤!")

                            # === Hook up AI Analyzer Context ===
                            unit_status_now = (
                                win_df.assign(is_ano=(win_df.get("anomaly", 1) == -1).astype(int))
                                .groupby("unit", as_index=False)
                                .agg(RUL=("RUL", "mean"), time=("time", "count"), anomaly_count=("is_ano", "sum"))
                            )
                            if len(unit_status_now) > 0:
                                unit_status_now["anomaly_rate"] = np.where(
                                    unit_status_now["time"] > 0,
                                    unit_status_now["anomaly_count"] / unit_status_now["time"] * 100, 0
                                )
                            else:
                                unit_status_now = pd.DataFrame(columns=["unit", "RUL", "time", "anomaly_count", "anomaly_rate"])

                            # ì»¨í…ìŠ¤íŠ¸ ê°±ì‹ 
                            st.session_state.analyzer.update_context(
                                df.assign(_is_window=df["time"].isin(win_df["time"]) & df["unit"].isin(win_df["unit"])),
                                unit_status=unit_status_now
                            )
                            st.session_state.analysis_complete = True

                            # ë””ë²„ê·¸ìš©(ì ê¹ í™•ì¸í•´ë³´ê³  í•„ìš”ì—†ìœ¼ë©´ ì§€ì›Œë„ ë¨)
                            st.caption(
                                f"snapshot%={snapshot_pct}, window={anom_window}, ì„ê³„ì¹˜={critical_rul_thresh} | "
                                f"df_units={df['unit'].nunique()}, snap_units={snap_df['unit'].nunique()}, win_rows={len(win_df)}"
                            )

                            # === Hook up AI Analyzer Context ===
                            st.session_state.analyzer.update_context(
                                df.assign(_is_window=df["time"].isin(win_df["time"]) & df["unit"].isin(win_df["unit"])),
                                unit_status=unit_status_now
                            )
                            st.session_state.analysis_complete = True

                            # Basic data info (snapshot/window)
                            st.markdown("### ğŸ“‹ ë°ì´í„° ì •ë³´")
                            i1, i2, i3 = st.columns(3)
                            with i1:
                                st.metric("ì´ ë ˆì½”ë“œ ìˆ˜(ìœˆë„ìš°)", len(win_df))
                            with i2:
                                st.metric("ì¥ë¹„ ìˆ˜", total_units)
                            with i3:
                                st.metric("ì„¼ì„œ ìˆ˜", len(selected_sensors))

                        except Exception as e:
                            st.error(f"ì´ìƒ íƒì§€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                            st.info("ê¸°ë³¸ í†µê³„ ë¶„ì„ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")

                            st.markdown("## ğŸ“Š ê¸°ë³¸ ë°ì´í„° ë¶„ì„")
                            c1, c2, c3, c4 = st.columns(4)
                            with c1:
                                st.metric("ì´ ì¥ë¹„", f"{int(df['unit'].nunique())}ëŒ€")
                            with c2:
                                st.metric("ì´ ë°ì´í„°", f"{len(df)}ê±´")
                            with c3:
                                st.metric("í‰ê·  RUL", f"{float(df['RUL'].mean()):.1f}")
                            with c4:
                                st.metric("ìœ„í—˜ ì¥ë¹„", f"{int(df[df['RUL'] < 30]['unit'].nunique())}ëŒ€")

                            st.session_state.analyzer.update_context(df, None)
                            st.session_state.analysis_complete = True
                    else:
                        st.error("âŒ ë¶„ì„ ê°€ëŠ¥í•œ ì„¼ì„œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
                        st.info("ë°ì´í„°ì— ë³€ë™ì„±ì´ ìˆëŠ” ìˆ«ìí˜• ì„¼ì„œ ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                        st.markdown("### ğŸ“‹ ë°ì´í„° êµ¬ì¡°")
                        st.write(f"ë°ì´í„° í¬ê¸°: {df.shape}")
                        st.write(f"ì»¬ëŸ¼: {list(df.columns)}")
                        st.markdown("### ğŸ‘€ ìƒ˜í”Œ ë°ì´í„°")
                        st.dataframe(df.head())
                except Exception as e:
                    st.error(f"RUL ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                    st.info("ê¸°ë³¸ ë°ì´í„° í‘œì‹œë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
                    st.markdown("### ğŸ“‹ ë¡œë“œëœ ë°ì´í„°")
                    st.write(f"ë°ì´í„° í¬ê¸°: {df.shape}")
                    st.dataframe(df.head(10))
            else:
                st.error("âŒ 'unit'ê³¼ 'time' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤!")
                st.info("ë°ì´í„°ì˜ ì²« ë²ˆì§¸ ì»¬ëŸ¼ì€ 'unit', ë‘ ë²ˆì§¸ ì»¬ëŸ¼ì€ 'time'ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
                st.markdown("### ğŸ“‹ í˜„ì¬ ë°ì´í„° êµ¬ì¡°")
                st.write(f"ì»¬ëŸ¼: {list(df.columns)}")
                st.dataframe(df.head())
        else:
            st.error("âŒ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        st.error(f"âŒ ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        st.error("íŒŒì¼ í˜•ì‹ê³¼ ë‚´ìš©ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        with st.expander("ğŸ”§ ë””ë²„ê¹… ì •ë³´"):
            st.write(f"ì˜¤ë¥˜ ìœ í˜•: {type(e).__name__}")
            st.write(f"ì˜¤ë¥˜ ë©”ì‹œì§€: {str(e)}")
            if hasattr(uploaded_file, 'name'):
                st.write(f"íŒŒì¼ëª…: {uploaded_file.name}")
            if hasattr(uploaded_file, 'size'):
                st.write(f"íŒŒì¼ í¬ê¸°: {uploaded_file.size} bytes")
else:
    # Landing Section
    st.markdown(
        """
        <div style="text-align: center; padding: 3rem 0;">
            <h2>ğŸš€ ìŠ¤ë§ˆíŠ¸íŒ©í† ë¦¬ AI ë¶„ì„ ì‹œìŠ¤í…œ</h2>
            <p style="font-size: 1.2rem; color: #666;">ì„¼ì„œ ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•˜ì—¬ ì§€ëŠ¥í˜• ì´ìƒ íƒì§€ ë° AI ë¶„ì„ì„ ì‹œì‘í•˜ì„¸ìš”</p>
        </div>
        """,
        unsafe_allow_html=True,
    )
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        st.markdown(
            """
            ### âœ¨ ì£¼ìš” ê¸°ëŠ¥
            
            ğŸ” **ì‹¤ì‹œê°„ ì´ìƒ íƒì§€**  
            - Isolation Forest ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì´ìƒ íŒ¨í„´ ìë™ ê°ì§€
            - ì¥ë¹„ë³„ ì‹¤ì‹œê°„ ìƒíƒœ ëª¨ë‹ˆí„°ë§
            
            ğŸ“Š **ì¸í„°ë™í‹°ë¸Œ ì‹œê°í™”**  
            - Plotly ê¸°ë°˜ ë™ì  ì°¨íŠ¸
            - ì¥ë¹„ ìƒíƒœ íˆíŠ¸ë§µ ë° íŠ¸ë Œë“œ ë¶„ì„
            
            ğŸ¤– **AI ë°ì´í„° ë¶„ì„ê°€**  
            - ìì—°ì–´ë¡œ ë°ì´í„° ì§ˆì˜ì‘ë‹µ
            - ìë™ ì¸ì‚¬ì´íŠ¸ ìƒì„± ë° ê¶Œê³ ì‚¬í•­ ì œì‹œ
            
            ğŸ’¡ **ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸**  
            - ì˜ˆë°©ì •ë¹„ ìš°ì„ ìˆœìœ„ ì œì‹œ  
            - ROI ê¸°ë°˜ ì˜ì‚¬ê²°ì • ì§€ì›
            """
        )
        st.markdown("### ğŸ“‹ ì§€ì› ë°ì´í„° í˜•ì‹")
        st.code(
            """
            unit  time  os1  os2  os3  sensor_1  sensor_2  ...  sensor_21
            1     1     -0.1  0.2  0.5   518.67   641.82          2388.02
            1     2     -0.2  0.1  0.4   518.67   642.15          2388.07
            ...
            """
        )
        st.info("ğŸ‘ˆ ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ë°ì´í„° íŒŒì¼ì„ ì„ íƒí•˜ê±°ë‚˜ ì—…ë¡œë“œí•˜ì—¬ ì‹œì‘í•˜ì„¸ìš”!")
        st.markdown("---")
        st.markdown("### ğŸ’¡ ë°ì´í„° ë¶„ì„ íŒ")
        c1, c2 = st.columns(2)
        with c1:
            st.markdown(
                """
                **ğŸš‚ Train ë°ì´í„° íŠ¹ì§•:**
                - FD001: 100ê°œ ì¥ë¹„, 21ê°œ ì„¼ì„œ
                - FD002: 260ê°œ ì¥ë¹„, 21ê°œ ì„¼ì„œ  
                - FD003: 100ê°œ ì¥ë¹„, 21ê°œ ì„¼ì„œ
                - FD004: 249ê°œ ì¥ë¹„, 21ê°œ ì„¼ì„œ
                
                **ğŸ“Š ë°ì´í„° êµ¬ì¡°:**
                - unit: ì¥ë¹„ ID
                - time: ì‚¬ì´í´ ì‹œê°„
                - os1~3: ìš´ì˜ ì„¤ì •
                - sensor_1~21: ì„¼ì„œ ê°’
                """
            )
        with c2:
            st.markdown(
                """
                **ğŸ” ë¶„ì„ ìˆœì„œ:**
                1. ë°ì´í„° í´ë”ì—ì„œ train íŒŒì¼ ì„ íƒ
                2. ìë™ ì»¬ëŸ¼ ì¸ì‹ ë° RUL ê³„ì‚°
                3. ì´ìƒ íƒì§€ ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰
                4. ì‹œê°í™” ë° AI ë¶„ì„
                
                **âš ï¸ ì£¼ì˜ì‚¬í•­:**
                - ëŒ€ìš©ëŸ‰ íŒŒì¼ì€ ë¡œë”© ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŒ
                - ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ íŒŒì¼ í¬ê¸° í™•ì¸ í•„ìš”
                """
            )

# =============================
# AI Assistant Tabs
# =============================
st.markdown("---")
main_tabs = st.tabs(["ğŸ’¬ AI ëŒ€í™”", "ğŸ“ˆ ì¸ì‚¬ì´íŠ¸", "â“ ì˜ˆìƒ ì§ˆë¬¸"])

with main_tabs[0]:
    st.markdown("## ğŸ’¬ AI ë¶„ì„ê°€ì™€ ëŒ€í™”í•˜ê¸°")

    # Sample data loader for quick testing
    cent1, cent2, cent3 = st.columns([1, 2, 1])
    with cent2:
        if st.button("ğŸ“ ìƒ˜í”Œ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸", use_container_width=True):
            np.random.seed(42)
            units = list(range(1, 11))
            rows: List[Dict[str, Any]] = []
            for unit in units:
                cycles = np.random.randint(50, 200)
                for cycle in range(1, cycles + 1):
                    rul = cycles - cycle
                    sensors = {}
                    for i in range(1, 22):
                        base_value = np.random.normal(500, 50)
                        noise = np.random.normal(0, 10)
                        degradation = cycle * 0.1
                        sensors[f'sensor_{i}'] = base_value + noise + degradation
                    rows.append({'unit': unit, 'time': cycle, 'RUL': rul, **sensors})
            sdf = pd.DataFrame(rows)
            sdf['anomaly'] = np.random.choice([-1, 1], len(sdf), p=[0.1, 0.9])
            ust = sdf.groupby('unit').agg({'RUL': 'mean', 'time': 'count'}).reset_index()
            anomaly_counts = sdf[sdf['anomaly'] == -1].groupby('unit').size().reset_index(name='anomaly_count')
            ust = ust.merge(anomaly_counts, on='unit', how='left')
            ust['anomaly_count'] = ust['anomaly_count'].fillna(0)
            ust['anomaly_rate'] = (ust['anomaly_count'] / ust['time'] * 100)
            st.session_state.analyzer.update_context(sdf, ust)
            st.session_state.analysis_complete = True
            st.success("âœ… ìƒ˜í”Œ ë°ì´í„°ê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
            st.rerun()

    if st.session_state.analysis_complete:
        st.markdown("### ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")
        chat_container = st.container()
        with chat_container:
            for chat in st.session_state.chat_history:
                if chat['type'] == 'user':
                    st.markdown(f"""
                    <div class="user-message">{chat['content']}</div>
                    """, unsafe_allow_html=True)
                else:
                    st.markdown(f"""
                    <div class="ai-message">{chat['content']}</div>
                    """, unsafe_allow_html=True)

        user_question = st.text_input(
            "ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", placeholder="ì˜ˆ: ê°€ì¥ ìœ„í—˜í•œ ì¥ë¹„ëŠ” ë¬´ì—‡ì¸ê°€ìš”?", key="user_input"
        )
        col_a, col_b = st.columns([1, 4])
        with col_a:
            if st.button("ğŸ’¬ ì§ˆë¬¸í•˜ê¸°", use_container_width=True):
                if user_question:
                    st.session_state.chat_history.append({
                        'type': 'user', 'content': user_question, 'timestamp': datetime.now()
                    })
                    answer = st.session_state.analyzer.analyze_question(user_question)
                    if st.session_state.use_llm and api_key:
                        answer = llm_answer(user_question, st.session_state.analyzer.analysis_context, api_key)
                    st.session_state.chat_history.append({
                        'type': 'ai', 'content': answer, 'timestamp': datetime.now()
                    })
                    st.rerun()
        with col_b:
            if st.button("ğŸ—‘ï¸ ëŒ€í™” ê¸°ë¡ ì‚­ì œ", use_container_width=True):
                st.session_state.chat_history = []
                st.rerun()
    else:
        st.info("ğŸ“ ë¨¼ì € ì„¼ì„œ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì„¸ìš”. ìœ„ì˜ 'ìƒ˜í”Œ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸' ë²„íŠ¼ì„ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.")

with main_tabs[1]:
    st.markdown("## ğŸ“ˆ AI ìƒì„± ì¸ì‚¬ì´íŠ¸")
    if st.session_state.analysis_complete:
        insights = st.session_state.analyzer.generate_insights()
        if insights:
            st.markdown("### ğŸ” ì£¼ìš” ë°œê²¬ì‚¬í•­")
            for i, insight in enumerate(insights):
                st.markdown(
                    f"""
                    <div class="insight-card">
                        <strong>ì¸ì‚¬ì´íŠ¸ {i+1}</strong><br>{insight}
                    </div>
                    """,
                    unsafe_allow_html=True,
                )
        else:
            st.success("ğŸ‰ í˜„ì¬ ì‹œìŠ¤í…œ ìƒíƒœê°€ ì–‘í˜¸í•©ë‹ˆë‹¤!")

        st.markdown("### ğŸ“Š ìƒì„¸ ë¶„ì„ ê²°ê³¼")
        ctx = st.session_state.analyzer.analysis_context
        c1, c2 = st.columns(2)
        with c1:
            st.markdown(
                """
                <div class="analysis-summary">
                    <h4>ğŸ­ ì¥ë¹„ í˜„í™©</h4>
                    <ul>
                        <li>ì´ ì¥ë¹„ ìˆ˜: {total_units}ëŒ€</li>
                        <li>ì „ì²´ ë°ì´í„°: {total_records:,}ê±´</li>
                        <li>ì„¼ì„œ ìˆ˜: {sensor_count}ê°œ</li>
                    </ul>
                </div>
                """.format(
                    total_units=ctx.get('total_units', 0),
                    total_records=ctx.get('total_records', 0),
                    sensor_count=len(ctx.get('sensor_columns', [])),
                ),
                unsafe_allow_html=True,
            )
        with c2:
            st.markdown(
                """
                <div class="analysis-summary">
                    <h4>âš ï¸ ìœ„í—˜ë„ ë¶„ì„</h4>
                    <ul>
                        <li>í‰ê·  RUL: {avg_rul:.1f} ì‚¬ì´í´</li>
                        <li>ìœ„í—˜ ì¥ë¹„: {critical_units}ëŒ€</li>
                        <li>ì´ìƒ ì§•í›„ìœ¨: {anomaly_rate:.2f}%</li>
                    </ul>
                </div>
                """.format(
                    avg_rul=ctx.get('avg_rul', 0),
                    critical_units=ctx.get('critical_units', 0),
                    anomaly_rate=ctx.get('anomaly_rate', 0),
                ),
                unsafe_allow_html=True,
            )
    else:
        st.info("ğŸ“Š ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê±°ë‚˜ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")

with main_tabs[2]:
    st.markdown("## â“ ì˜ˆìƒ ì§ˆë¬¸ ë° ë¹ ë¥¸ ë¶„ì„")
    if st.session_state.analysis_complete:
        questions = st.session_state.analyzer.generate_questions()
        st.markdown("### ğŸ¤” ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ë“¤")
        st.markdown("ì•„ë˜ ì§ˆë¬¸ë“¤ì„ í´ë¦­í•˜ë©´ ì¦‰ì‹œ ë‹µë³€ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
        for i, q in enumerate(questions):
            if st.button(f"â“ {q}", key=f"q_{i}", use_container_width=True):
                st.session_state.chat_history.append({'type': 'user', 'content': q, 'timestamp': datetime.now()})
                ans = st.session_state.analyzer.analyze_question(q)
                if st.session_state.use_llm and api_key:
                    ans = llm_answer(q, st.session_state.analyzer.analysis_context, api_key)
                st.session_state.chat_history.append({'type': 'ai', 'content': ans, 'timestamp': datetime.now()})
                st.rerun()
        st.markdown("---")
        st.markdown("### ğŸ’¡ ë¶„ì„ íŒ")
        st.info(
            """
            **íš¨ê³¼ì ì¸ ì§ˆë¬¸ ì˜ˆì‹œ:**
            - "RULì´ 10 ë¯¸ë§Œì¸ ì¥ë¹„ë“¤ì˜ íŠ¹ì§•ì€?"
            - "ì„¼ì„œ 5ë²ˆì˜ ì´ìƒ íŒ¨í„´ì„ ë¶„ì„í•´ì£¼ì„¸ìš”"  
            - "ì˜ˆë°©ì •ë¹„ ë¹„ìš© ëŒ€ë¹„ íš¨ê³¼ëŠ”?"
            - "ì¥ë¹„ë³„ ê³ ì¥ ìœ„í—˜ë„ ìˆœìœ„ëŠ”?"
            """
        )
    else:
        st.info("â“ ì§ˆë¬¸ ëª©ë¡ì„ ìƒì„±í•˜ë ¤ë©´ ë¨¼ì € ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”. ë˜ëŠ” ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
